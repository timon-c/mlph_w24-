{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Optimal Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = 5\n",
    "num_sources = 10\n",
    "num_sinks = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "mass_sources = np.random.random(num_sources)\n",
    "mass_sinks = np.random.random(num_sinks)\n",
    "mass_sources /= np.sum(mass_sources)\n",
    "mass_sinks /= np.sum(mass_sinks)\n",
    "\n",
    "coords_sources = np.random.rand(num_sources, d)\n",
    "coords_sinks = np.random.rand(num_sinks, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve the OT problem as linear program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Flow matching for generative modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_checkerboard_sample(num_samples=10, field_size=0.4, num_fields=2, center=True):\n",
    "    x = torch.rand(num_samples, 2) * field_size\n",
    "    offset = torch.randint(0, num_fields, (num_samples, 2)) * field_size * 2\n",
    "    diagonal_shift = torch.randint(0, num_fields, (num_samples, 1)) * field_size\n",
    "    x += offset + diagonal_shift\n",
    "\n",
    "    if center:\n",
    "        x -= torch.mean(x, dim=0)\n",
    "\n",
    "    return x\n",
    "    \n",
    "base_distribution_std = 0.15\n",
    "num_samples = 2000\n",
    "x = torch.randn(num_samples, 2) * base_distribution_std\n",
    "y = generate_checkerboard_sample(num_samples=num_samples)\n",
    "\n",
    "# show points\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=0.5, label='base distribution')\n",
    "plt.scatter(y[:, 0], y[:, 1], alpha=0.5, label='checkerboard distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "from torchvision.ops import MLP\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = MLP(in_channels=2 + 1, hidden_channels=[512, 512, 512, 512, 2], activation_layer=torch.nn.SiLU)\n",
    "model.to(device)\n",
    "\n",
    "# define a loss function\n",
    "criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# train the model:\n",
    "num_epochs = 20000  # use fewer epochs if it takes too long\n",
    "batch_size = 4096\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    x = torch.randn(batch_size, 2) * base_distribution_std\n",
    "    y = generate_checkerboard_sample(num_samples=batch_size)\n",
    "\n",
    "    # TODO: implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run inference with the trained model. \n",
    "# Visualize the trajectory of the samples and the final samples at t=1.\n",
    "# Hint: Use a simple Euler integration scheme to integrate the velocity field with 100 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adversarial attacks and AI safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Logistric regression in pytorch (needed for backpropagation)\n",
    "taken from https://github.com/saprmarks/geometry-of-truth/blob/main/probes.py\n",
    "'''\n",
    "\n",
    "class LRProbe(torch.nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_in, 1, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def pred(self, x):\n",
    "        return self(x).round()\n",
    "    \n",
    "    def from_data(acts, labels, lr=0.001, weight_decay=0.1, epochs=1000, device='cpu'):\n",
    "        acts, labels = acts.to(device), labels.to(device)\n",
    "        probe = LRProbe(acts.shape[-1]).to(device)\n",
    "        \n",
    "        opt = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        for _ in range(epochs):\n",
    "            opt.zero_grad()\n",
    "            loss = torch.nn.BCELoss()(probe(acts), labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        return probe\n",
    "\n",
    "    def __str__():\n",
    "        return \"LRProbe\"\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self.net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the DataManager class as a helper function to load the activation vectors for us.\n",
    "from lie_detection_utils import DataManager\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "path_to_datasets = \"data/lie_detection/datasets\"\n",
    "path_to_acts = \"data/lie_detection/acts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model on the cities dataset\n",
    "dataset_name = \"cities\"\n",
    "\n",
    "dm = DataManager()\n",
    "dm.add_dataset(dataset_name, \"Llama3\", \"8B\", \"chat\", layer=12, split=0.8, center=False,\n",
    "                device='cpu', path_to_datasets=path_to_datasets, path_to_acts=path_to_acts)\n",
    "train_acts, train_labels = dm.get('train')\n",
    "test_acts, test_labels = dm.get('val')\n",
    "\n",
    "print(\"train_acts.shape\", train_acts.shape)\n",
    "print(\"test_acts.shape\", test_acts.shape)\n",
    "\n",
    "# TODO: train a logistic regression probe on the train_acts and train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: optimize a perturbation on a single sample which is a lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check whether this perturbation works on other samples too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add the constraint that the perturbation should be small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

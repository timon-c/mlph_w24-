{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Logistic regression: an LLM lie detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from https://heibox.uni-heidelberg.de/f/38bd3f2a9b7944248cc2/   \n",
    "Unzip it and place the lie_detection folder in the folder named `data` to get the following structure:\n",
    "\"data/lie_detection/datasets\" and \"data/lie_detection/acts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can load a dataset of LLM activations. Use a new Datamanager if you want to have a new dataset. Use the same data manager if you want to combine datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lie_detection_utils import DataManager\n",
    "\n",
    "path_to_datasets = \"data/lie_detection/datasets\"\n",
    "path_to_acts = \"data/lie_detection/acts\"\n",
    "\n",
    "# check if the datasets and activations are available\n",
    "assert os.path.exists(path_to_datasets), \"The path to the datasets does not exist.\"\n",
    "assert os.path.exists(path_to_acts), \"The path to the activations does not exist.\"\n",
    "\n",
    "# these are the different datasets containing true and false factual statements about different topics\n",
    "dataset_names = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\"]\n",
    "dataset_name = dataset_names[0] # choose some dataset from the above datasets, index \"0\" loads the \"cities\" dataset for example\n",
    "\n",
    "# the dataloader automatically loads the training data for us\n",
    "dm = DataManager()\n",
    "dm.add_dataset(dataset_name, \"Llama3\", \"8B\", \"chat\", layer=12, split=0.8, center=False,\n",
    "                device='cpu', path_to_datasets=path_to_datasets, path_to_acts=path_to_acts)\n",
    "acts_train, labels_train = dm.get('train') # train set\n",
    "acts_test, labels_test = dm.get('val')\n",
    "print(acts_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the statements that were fed to the LLM to produce the activations:\n",
    "df = pd.read_csv(f\"{path_to_datasets}/{dataset_name}.csv\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Log-sum-exp and soft(arg)max\n",
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Linear regions of MLPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
